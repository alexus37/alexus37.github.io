<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" class=" social-js">
<head profile="http://gmpg.org/xfn/11">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>
<title>Alexus Lexus</title>

<link href="../images/favicon.ico" rel="shortcut icon">

<link rel="stylesheet" href="../css/style.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/animation.css" type="text/css" media="screen">

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/css/bootstrap.min.css">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/css/bootstrap-theme.min.css">
<link rel="stylesheet" type="text/css" href="css/diffWidget.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
<script src="js/diffWidget.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>

<script type="text/javascript">
	$(document).ready(function(){
           $('li img').on('click',function(){
                var src = $(this).attr('src');
                var img = '<img src="' + src + '" class="img-responsive"/>';
                $('#myModal').modal();
                $('#myModal').on('shown.bs.modal', function(){
                    $('#myModal .modal-body').html(img);
                });
                $('#myModal').on('hidden.bs.modal', function(){
                    $('#myModal .modal-body').html('');
                });
           });  
        })
</script>
<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>




</head>

<body id="home">
	<div id="wrapper">
		<div id="container-work">	
			<div id="lnav">
				<ul>
					<li id="pages-2" class="widget widget_pages"><h2 class="widgettitle">Navigation</h2>		
						<ul>
							<li class="page_item page-item-34"><a href="home.html">Home</a></li>
							<li class="page_item page-item-3"><a href="about.html">About</a></li>
							<li class="page_item page-item-5"><a href="art.html">Art</a></li>
							<li class="page_item page-item-2"><a href="projects.html">Projects</a></li>
							<li class="page_item page-item-2 current_page_item"><a href="bsThesisAX.html">Thesis</a></li>             
                            <li class="page_item page-item-2"><a href="publications.html">Publications</a></li>
                            <li class="page_item page-item-2"><a href="travelling.html">Travelling</a></li>				
						</ul>
					</li>
				</ul>
			</div>
			<div id="rcontent">
				<div id="thesisHeader">
					<h1>Structure-aware Surface Reconstruction with Sparse Moving Least Squares</h1>						 
				</div>
				<div id="thesisSuperviser">
					<h2>from Alexander Lelidis supervised by Cengiz Oztireli and Marc Alexa</h2>						 
				</div>
					
				<div id="thesiscontent">
					<div class="panel-group" id="accordion">
						<div class="panel panel-default">
				            <div class="panel-heading">
				                <h3 class="panel-title">
				                    <a data-toggle="collapse" data-parent="#accordion" href="#introduction">Introduction</a>
				                </h3>
				            </div>
				            <div id="introduction" class="panel-collapse collapse in">
				                <div class="panel-body">
				                	<div class="col-md-6">
					                   	<p class="text">
											Reconstructing the surface underlying a given point cloud is a fundamental problem in geometry
											processing. Moving least squares solves this problem efficiently using local fits. However, locality
											comes at the expense of losing a global view of the geometry, leading to inferior results when
											there is missing data or significant amount of noise or outliers. Global methods are more robust,
											but they are expensive to compute. In this thesis, we will combine global and local methods in
											an efficient manner by using learned local geometry bases and sparse moving least squares fits.
										</p>
									</div>
									<div class="col-md-6">
										<div class="thumbnail">
											<ul class="row">
												<li class="thumb">
								         			<img src="images/bunnyTrans.png" height="650" width="750" class="img-responsive" alt="Responsive image">
								         		</li>
							      			</ul>
							      		</div>
                                        <a href="data/thesis.pdf" download><img src="images/pdf-flat.png"><h4>Download as PDF-file</h4></a>
									</div>
				                </div>
				            </div>
				        </div>

				        <div class="panel panel-default">
				            <div class="panel-heading">
				                <h3 class="panel-title">
				                    <a data-toggle="collapse" data-parent="#accordion" href="#basicIdea">Basic idea</a>
				                </h3>
				            </div>
				            <div id="basicIdea" class="panel-collapse collapse">
				                <div class="panel-body">
				                    <h3>Moving least squares (MLS)</h3>
										<p class="text">
											The whole algorithm will build on moving least squares defined in the following part.
											The input will be a point cloud of a scanned real world object, optionally with assigned
											normals. Further our input point cloud will be called \(P \). We want to compute an implicit function, which zero set defines the surface of the scanned
											mesh. By its nature the function should be globally defined. For every input point \(x_i\) with \( i \in [0 ... N]\) 
											the function should minimize the error to the assigned function values \(f_i\) in a least square sense.
											$$
											LS_{error} = min \sum\limits_0^N || f(x_i) - y_i ||^2
											$$
											The function is taken from a given polynomial space of degree m. The function can be written as 
											$$
											f(x) = b(x) \cdot c
											$$
											where \(b(x)\) is a vector of basis elements evaluated for the given point \(x\). For example for degree 
											1 b equals \([1, x, y, z]^T\). c are the unknown coefficients, which should be minimized. 
											To be able to reduce the influence of points which are far away from our current inspected point \(x\) we use
											weights, depending on the distance between \(x\) and \(x_i\) and we only use the k-nearest neighbours for the computation.
											$$
											MLS_{error} = \min \sum\limits_{i = 0}^{k} \theta(||x - x_i||) || f(x_i) - y_i ||^2
											$$ 
											The scanned points \(x_i\) should lay approximately on the surface, so the result of the implicit function should be zero 
											at this points. 
										</p>

									<h3>Using a dictionary for the coefficients c</h3>
										<p class="text">
											Our approach is to replace the coefficients c by a dictionary \(D\) multiplied by a dictionary coefficients vector \(\alpha\). In the beginning \(D\) contains  some polynomials, which are usually good for covering features of the surface. This dictionary will also be trained by the K-SVD algorithm to get better fitting elements.
										</p>
									<h3>Finding the optimal \(\alpha\)</h3>
										<p class="text">
											To improve the performance and reduce the noise we want to use a sparse representation for \(\alpha\) according to the L1-Norm. Typically surface region from real world data can be covered by a few underlying basis or in our case dictionary elements. This fact we want to explore, by using a sparse representation.
											$$
											f(x) = \sum\limits_{i = 0}^m c_i b_i(x) = c^T b(x) = (D \alpha)^T b(x)
											$$
											with the constraint that \(\alpha\) should be sparse according to some threshold \(\epsilon\) and bigger than zero to avoid the trivial solution.
											$$
											||\alpha||_1 < \epsilon \wedge \alpha \neq 0
											$$ 
											However the final term which should be minimized is 
											$$
											\min\limits_{||\alpha||_1 < \epsilon \wedge \alpha \neq 0}^{} \sum\limits_{i = 0}^{k} \theta(||x - x_i||) || \alpha^T D^T b(x_i) - y_i||^2
											$$
										</p>
				                </div>
				            </div>
				        </div>
				        <div class="panel panel-default">
				            <div class="panel-heading">
				                <h3 class="panel-title">
				                    <a data-toggle="collapse" data-parent="#accordion" href="#firstSteps">First steps</a>
				                </h3>
				            </div>
				            <div id="firstSteps" class="panel-collapse collapse">
				                <div class="panel-body">
				                    <h3>Basic implementation in C++</h3>
										<p class="text">
 											I started implementing a basic moving least squares according to 
 											<a href="http://www.labri.fr/perso/guenneba/docs/APSS_sig07.pdf">Algebraic Point Set Surfaces </a>. To not
 											reinvent the wheel I use the <a href="http://eigen.tuxfamily.org/index.php?title=Main_Page">Eigen</a> 
 											library for linear algebra and <a href="https://github.com/libigl/libigl">libigl</a> for basic 
 											visualization and simple mesh processing algorithms like Marching cubes.
										</p>
									<h3>Current features</h3>
										<p class="text">
											The current project does not have a lot features, but there still are few to mention.
											<table class="tg" style="font-weight: normal; table-layout: fixed; width: 500px" >
												<colgroup>
													<col style="width: 170px">
													<col style="width: 550px">
												</colgroup>
												<tr>
													<td class="tg-031e">Basis</td>
													<td class="tg-031e" style="font-weight: normal;">There are polynomial basis implemented with of degree N. They could be changed in the GUI.</td>

												</tr>
												<tr>
													<td class="tg-031e">Weights</td>
													<td class="tg-031e" style="font-weight: normal;">
														The weighting function is the Wendland function. The wendland radius is computed by the length of the diagonal of the 
														bounding box of the geometry and a parameter specified by the user (Default 0.1). The parameter could also be changed 
														in the GUI.
													</td>
												</tr>
												<tr>
													<td class="tg-031e">Marching Cubes</td>
													<td class="tg-031e" style="font-weight: normal;">The resolution of the grid for the Marching Cubes algorithm could also be set in the GUI.</td>
												</tr>
												<tr>
													<td class="tg-031e">PCA</td>
													<td class="tg-031e" style="font-weight: normal;">Some meshes are not really aligned the Euclidean axis, it makes sense to get the principal
													components and transform all input points and normals to get better results with the Marching cubes algorithm. The option could also be set in the GUI</td>
												</tr>
												<tr>
													<td class="tg-031e">Additonal points</td>
													<td class="tg-031e" style="font-weight: normal;">Sometimes it is useful to create additional points which serve as constraints. The points are generated in the 
													following way: <br>
													<br>
													Compute a \(\epsilon \) according to the bounding box
													For every point \(p_i\)
													<ol  style="margin-left: 40px;">
													  <li>\(p_{i1} = p_i + \epsilon n_i\)</li>
													  <li>\(p_{i2} = p_i - \epsilon n_i\)</li>
													  <li>Check if there is no point closerer to \(p_i\) then \(p_{i1}\) or \(p_{i2}\). If so, repeat the process with the halved \(\epsilon \)</li>
													</ol>

													</td>
												</tr>
												<tr>
													<td class="tg-031e">Solver</td>
													<td class="tg-031e" style="font-weight: normal;">The current solver minimizes L2- Norm without any constraints. As a solver I use
													the <a href="http://eigen.tuxfamily.org/dox-devel/classEigen_1_1LDLT.html"> ldlt</a> solver from Eigen.</td>
												</tr>
											</table>
										</p>
									<h4>Results</h4>
										<p class="text">
											The following images were created using 1000 points on the Stanford bunny. 
											As a resolution for the Marching cubes I used 20 x 20 x 20 and a wendland radius of 0.1.
										</p>
										<h3>Comparison between degree 0 and 1</h3>
										<div id="results1"></div>

										<h3>Comparison between degree 1 and 2</h3>
										<div id="results2"></div>
										
									<h4>Next steps</h4>
									<p>
										The next steps are to add a solver with a L1-Norm constraint. Implement a few more basis and weighting functions.
									</p>
				                </div>
				            </div>
				        </div>
				        <div class="panel panel-default">
				            <div class="panel-heading">
				                <h3 class="panel-title">
				                    <a data-toggle="collapse" data-parent="#accordion" href="#KL1p">Compressed sensing experiments</a>
				                </h3>
				            </div>
				            <div id="KL1p" class="panel-collapse collapse">
				                <div class="panel-body">
				                    <h3>Testing the open source library <a href="http://kl1p.sourceforge.net/home.html"> KL1p </a></h3>
				                    <p>
				                    	To achieve a sparse signal representation we want to min the following energy: 
				                    	$$
											min ||Ax - y||_2 s.t. ||x||_1 < \epsilon 
										$$ 
										To compute this nontrival minimization we are planing to use the KL1p library, which provides a few 
										iterative solvers. In the next part we are going to compare a 256 parameter long signal \( x_0 \), which contains 25 non zero elements with the reconstructed one. Therefor we create a random gaussian Matrix \(A^{128  \times 256} \) with a mean of 0 and deviation of 1, which we are going to use to create a measurement vector \(y = Ax_0\). The following images compare the original signal \( x_0 \) to the reconstructed  signal by using different solvers. For OMP, ROMP, CoSaMP, SubspacePursuita and EMBP we set
										\(\epsilon = 0.1 \cdot n \)
				                    </p>
									<h3>Comparison between orignal and BasisPursuit</h3>
									<p>
										SNR=111.85 - Time=86ms - Iterations=11
										<div id="BasisPursuit"></div>
									</p>
									<h3>Comparison between orignal and OMP</h3>
									<p>
										SNR=108.2 - Time=18ms - Iterations=25
										<div id="OMP"></div>
									</p>

									<h3>Comparison between orignal and ROMP</h3>
									<p>
										SNR=9.1739 - Time=2ms - Iterations=3
										<div id="ROMP"></div>
									</p>

									<h3>Comparison between orignal and CoSaMP</h3>
									<P>
										SNR=92.607 - Time=9ms - Iterations=4
										<div id="CoSaMP"></div>
									</P>
									<h3>Comparison between orignal and SubspacePursuit</h3>
									<p>
										SNR=51.934 - Time=13ms - Iterations=5
										<div id="SubspacePursuit"></div>
									</p>

									<h3>Comparison between orignal and SL0</h3>
									<p>
										SNR=62.913 - Time=86ms - Iterations=12
										<div id="SL0"></div>
									</p>

									<h3>Comparison between orignal and AMP</h3>
									<p>
										SNR=75.745 - Time=6ms - Iterations=71
										<div id="AMP"></div>
									</p>

									<h3>Comparison between orignal and EMBP</h3>
									<p>
										SNR=76.491 - Time=6ms - Iterations=56
										<div id="EMBP"></div>
									</p>
				                </div>
				            </div>
				        </div>
				        <div class="panel panel-default">
				            <div class="panel-heading">
				                <h3 class="panel-title">
				                    <a data-toggle="collapse" data-parent="#accordion" href="#speedUP">Speed up</a>
				                </h3>
				            </div>
				            <div id="speedUP" class="panel-collapse collapse">
				                <div class="panel-body">
				                    <p class="text">
					                	With the aim to compute the results faster, also for bigger meshes, I started to consider ways to optimize my basic framework. In the end of the section you can see a bar chart which compares the
					                	new implementation with the old one without any optimization in terms of run time. 
					                </p>
					                <h3>KD - Tree</h3>
					                <div class="col-md-6">
						                <p class="text">
						                	As mentioned in the previous section, I added a kd- tree. In the old framework I used
						                	simple uniform binning, which I replaced with the kd- tree. The kd tree is constructed during the initialization
						                	and is quicker for queries on large datasets, but also works well on small ones. 
						                </p>
						            </div>
						            <div class="col-md-6">
						            	<div class="thumbnail">
											<ul class="row">
												<li class="thumb">
								         			<img src="images/kdtree.png" height="462" width="486" class="img-responsive" alt="Responsive image">
								         		</li>
							      			</ul>
							      		</div>
						            </div>
						            <h3>Precomputing the basis</h3>
						            <p class="text">
						            	In the olf framework I compute the basis matrix every time I solved the MLS system. I think it would be faster to precompute 
						            	the basis for every vertex as a preprocessing step. This would be a big speed up if the grid for the marching
						            	cubes is really dense and surface points get used multiple times.
						            </p>
						            <h3>Thread building blocks</h3>
						            <p class="text">
						            	The last optimization I did was to evaluate the MLS function for every grid point in parallel. Because the grid point evaluation is 
						            	independent of other grid points, it could be easily parallelized. 
						            </p>
						            <div class="col-md-12">
						            	<div class="thumbnail">
											<ul class="row">
												<li class="thumb">
								         			<img src="images/graphs/barChart.png" height="600" width="800" class="img-responsive" alt="Responsive image">
								         		</li>
							      			</ul>
							      		</div>
						            </div>

				                </div>
				            </div>
				        </div>
				        <div class="panel panel-default">
			            <div class="panel-heading">
			                <h3 class="panel-title">
			                    <a data-toggle="collapse" data-parent="#accordion" href="#apss">Algebraic point set surfaces</a>
			                </h3>
			            </div>
			            <div id="apss" class="panel-collapse collapse">
			                <div class="panel-body">
			                <p class="text">
			                	I implemented the algebraic point set surface reconstruction algorithm proposed by Gael Guennebaud and Markus Gross in 2007. 
			                	The main advantage of the algorithm is the fact that it takes also the surface normals into account. Basically it says that the gradient of
			                	the implicit function should be aligned with the normal. A nice side effect is that we get the mean curvature for free, which corresponds 
			                	to the radius of the fitted sphere. In the end we the get following linear system of equations:
		                	</p>
			                	$$
			                		W(x) = \begin{bmatrix}
			                			\ddots &             &  				  & 		&   				 & \\
			                			       & \omega_i(x) &     				  &         &   				 & \\  
			                			       &             &  \beta \omega_i(x) & 		&   				 & \\
			                			       &             &  				  & \ddots  &   				 & \\		                			       
	   		                			       &             &  				  &			&  \beta \omega_i(x) & \\	
	   		                			       &             &  				  &			&   				 &  \ddots 	   		                			       
			                		\end{bmatrix} ,  

			                		B(x) = \begin{bmatrix}
			                			\vdots & \vdots      & \vdots \\
			                		  	1 	   & p_i^T       & p_i^T p_i \\
			                			0	   & e_0^T       & 2 e_0^T p_i \\
			                			\vdots & \vdots      & \vdots \\
			                			0      & e_{d-1}^T   & 2 e_{d-1}^T p_i \\  
			                			\vdots & \vdots      & \vdots 	                			       
			                		\end{bmatrix},

			                		\alpha = \begin{bmatrix}
			                			\vdots \\
			                			0 \\
			                			e_0^T n_i \\
	             			       		\vdots \\
	             			       		e_{d-1}^T n_i \\
	             			       		\vdots 
			                		\end{bmatrix}
			                	$$ 
			                	<p class="text">
			                	where d is the dimension and \(e_i\) represents the i unit basis vector of our coordinate system.
			                	</p>
			                	<h3>Results</h3>
			                	<p class="text"> 
			                		In the following section I am going to compare the result of the APSS algorithm and a least squares fit with plus-minus epsilon constraint points.
		                		</p>
		                		<div class="col-md-6">
					                <div id="APSS1"></div>
					                left - least squares fit right - APSS
					            </div>
					            <div class="col-md-6">
					            	<div id="APSS2"></div>
					            	left - least squares fit right - APSS
					            </div>


			                </div>
			            </div>
			        </div>
				        <div class="panel panel-default">
				            <div class="panel-heading">
				                <h3 class="panel-title">
				                    <a data-toggle="collapse" data-parent="#accordion" href="#polyTest">Tests with polynomials</a>
				                </h3>
				            </div>
				            <div id="polyTest" class="panel-collapse collapse">
				                <div class="panel-body">
				                    <p class="text">
										In this section I tried to use some of the sparse solvers from the KL1p library for our minimization problem. It makes no sense to apply the \(L_1\) norm to the
										APSS algorithm, because the coefficients vector only has 5 entries, which is not enough. So I decided to use the high order polynomials in hopes that they would be sparse and achieve reasonable reconstructions. In the following images I used polynomials of degree 8 and the 1.000 points sampled from the 
										Stanford bunny without any noise added. 
									</p>
									<div class="col-md-6">
						            	<div class="thumbnail">
						            		<h3>Orthogonal Matching Pursuit (OMP)</h3>
											<ul class="row">
												<li class="thumb">
								         			<img src="images/results/bunnyOMP.png" height="450" width="450" class="img-responsive" alt="Responsive image">
								         		</li>
							      			</ul>
							      		</div>
						            </div>
						            <div class="col-md-6">
						            	<div class="thumbnail">
						            		<h3>Regularized OMP</h3>
											<ul class="row">
												<li class="thumb">
								         			<img src="images/results/bunnyROMP.png" height="450" width="450" class="img-responsive" alt="Responsive image">
								         		</li>
							      			</ul>
							      		</div>
						            </div>
						            <div class="col-md-6">
						            	<div class="thumbnail">
						            		<h3>Subspace Pursuit</h3>
											<ul class="row">
												<li class="thumb">
								         			<img src="images/results/bunnySP.png" height="450" width="450" class="img-responsive" alt="Responsive image">
								         		</li>
							      			</ul>
							      		</div>
						            </div>
						            <div class="col-md-6">
						            	<div class="thumbnail">
						            		<h3>Smoothed \(L_0\)</h3>
											<ul class="row">
												<li class="thumb">
								         			<img src="images/results/bunnySL0.png" height="450" width="450" class="img-responsive" alt="Responsive image">
								         		</li>
							      			</ul>
							      		</div>
						            </div>
									<p class="text">
										As we can see, all \(L_1\) solvers produce the same result, which is nice, because they all solve the same problem. Still the result is much worse than the result from the APSS algorithm. I think the main reason for this is that polynomial basis are not sparse enough
										in comparison with the length of the signal. 
									</p>
				                </div>
				            </div>
				        </div>
				        <div class="panel panel-default">
				            <div class="panel-heading">
				                <h3 class="panel-title">
				                    <a data-toggle="collapse" data-parent="#accordion" href="#heightField">Gaussian height field</a>
				                </h3>
				            </div>
				            <div id="heightField" class="panel-collapse collapse">
				                <div class="panel-body">
				                    <p class="text">
										Since the reconstruction with polynomials did not work so well, I guess that coefficients vector alpha is not sparse enough. 
										To get a sparser vector we need a different set off basis, which represents the surface better. A possible solution would be to 
										use a set of Gaussians on the local tangent plane and try to create the surface as a local height field.
									</p>
									<h3>1D function reconstruction with Gaussian RBF</h3>
						            <p class="text">
						            	To get a better feeling for the \(L_1\) solving algorithms and their parameters I will start with
						            	the 1D case and a trigonometric function. At first I will solve the following \(L_1\) minimization with 
						            	quadratic (\(L_2\) norm) constraints.
						            	$$
						            		min ||\alpha||^2_1  \; s.t.||K\alpha - f||^2_2 < \epsilon\;
						            	$$
						            	where \(K\) is the radial Basis matrix, defined as \(K_{i, j} = e^{\frac{-||x_i - x_j||}{\sigma^2}}\).
						            	In the following graphs I create 64 uniform distributed points in the interval from 0 to 6 and used \(\sigma^2 = 0.07\) and \(\epsilon = 1e-3\)
						            	for the RB matrix. As a signal I used the following trigonometric function \(f(x) = 2 \sin(3x) + 8 \cos(\frac{x}{2})\)					            	
						            </p>
						            <div class="col-md-4">
						            	<p class="text">
						            		We can clearly see the reconstructed signal matches the original quiet well. The introduced error
						            		by the reconstruction is \(1.0e-06\) and the reconstruction took 0.027668 seconds.
						            	</p>
						            </div>
						            <div class="col-md-8">
						            	<div id="1DSignal"></div>
						            </div>
						            <div class="col-md-4">
						            	<p class="text">
						            		This comparison shows the 64 coefficients for the RBF. The first image shows the coefficients
						            		if we solve the problem with a simple least squares solver and the second image shows the 
						            		coefficients if we use the sparse solver. It is visible that the small coefficients in the middle 
						            		of the signal converge to 0. 
						            	</p>
						            </div>
						            <div class="col-md-8">
						            	<div id="1DSignalCoeff"></div>
						            </div>
						            <h3>2D function reconstruction with Gaussian RBF</h3>
						            <p class="text">
										The 1D function reconstruction worked well, so the next step is to raise the dimension and check if the 
										performance is still good. A big problem in the 1D was to find the right parameters for the
										\(L_1\) solver, which is very sensitive. However, for the 2D reconstruction I used again a trigonometric
										function, this time in 2D:
										$$
											f(x, y) = \frac{\sin(\sqrt{x^2 + y^2})}{\sqrt{x^2 + y^2}}
										$$
										To create the following graphs I create a meshgrid from -10 to 10 with 4096 uniformly distributed samples and
										\(\sigma^2 = 0.2\) and \(\epsilon = 1e-3\) for the RB matrix.			
									</p>
									<div class="col-md-12">
						            	<p class="text">
						            		This comparison shows the orignal signal and the sparse signal reconstruction, which match quiet well.
						            		The reconstruction took 167.298803 seconds and the total error is \(1.0e-03\).
						            	</p>
						            </div>
						            <div class="col-md-12">
						            	<div id="2DSignal"></div>
						            </div>
						            <div class="col-md-12">
						            	<p class="text">
						            		This comparison shows the 4096 coefficients for the RBF. Again we compare the least squares solution 
						            		with the sparse one. For the given parameters the solver converges to the least squares solution. The total
						            		difference between the coefficient vectors is only 0.0682. 
						            	</p>
						            </div>
						            <div class="col-md-12">
						            	<div id="2DSignalCoeff"></div>
						            </div>
				                </div>
				            </div>
				        </div>
				        <div class="panel panel-default">
				            <div class="panel-heading">
				                <h3 class="panel-title">
				                    <a data-toggle="collapse" data-parent="#accordion" href="#LSL1">\(L_1\) - regularized least squares problems</a>
				                </h3>
				            </div>
				            <div id="LSL1" class="panel-collapse collapse">
				                <div class="panel-body">
				                    <p class="text">
										Because the \(L_1\) minimization with quadratic (\(L_2\) norm) constraint (L1QC) wasn't really stable and converged 
										for the wrong parameters to the least squares solution. I tried to solve the problem with a different approach, using the 
										following problem definition:
										$$
											min ||K\alpha - f||^2_2 + \lambda ||\alpha||_1
										$$
										where \(\lambda  \ge 0 \) is the regularization parameter. 
									</p>
									<h3>1D function reconstruction with Gaussian RBF</h3>
						            <p class="text">
						            	Again we start with the 1D case and the same function defined in the section above. In terms of parameters, 
						            	I used exactly the same ones as before, the new parameter \(\lambda\) is 0.000001.
						            </p>
						            <div class="col-md-4">
						            	<p class="text">
						            		We can clearly see the reconstructed signal matches the original quiet well. The introduced error
						            		by the reconstruction is 0.0109 and the reconstruction took 2.237272 seconds. So the reconstruction
						            		is a little bit slower as  the L1QC method, also the error is higher, but the method is much more stable 
						            		and finds a solution most of the time.
						            	</p>
						            </div>
						            <div class="col-md-8">
						            	<div id="1DSignalLSL1" style="padding-top: 125px;"></div>
						            </div>
						            <div class="col-md-4">
						            	<p class="text">
						            		On the right hand we compare the L1QC solution with the LSL1 solution. 
						            		In the background we see the LS solution. We observe that the LSL1 method
						            		even for such a small lambda produces a sparser solution than the L1QC method.
						            	</p>
						            </div>
						            <div class="col-md-8">
						            	<div id="1DSignalCoeffLSL1"></div>
						            </div>
						            <div class="col-md-4">
						            	<p class="text">
						            		Now we directly compare the two coefficient vectors on a smaller
						            		scale.
						            	</p>
						            </div>
						            <div class="col-md-8">
						            	<div id="LSL1VSL1QC"></div>
						            </div>
						            <h3>Making noise</h3>
						            <p class="text">
						            	As we could see the reconstruction worked well. Now we would like test the 
						            	behavior of the algorithm with added Gaussian white noise. 
						            </p>
						            <div class="col-md-4">
						            	<p class="text">
						            		Therefor we created a noise vector \(v\) with a \(SNR = 20\) and 
							            	added it to the signal \(f_{Noisy} = f + v\). Now we tried to reconstruct the original signal with Least squares
							            	and with LSL1.
						            	</p>
						            </div>
						            <div class="col-md-8">
						            	<div class="thumbnail">
											<ul class="row">
												<li class="thumb">
								         			<img src="images/graphs/1DNoise.png" height="384" width="512" class="img-responsive" alt="Responsive image">
								         		</li>
							      			</ul>
							      		</div>
						            </div>
						            <div class="col-md-4">
						            	<p class="text">
						            		We can observe the expected behavior, that the least squares algorithm tries exactly to match the data. 
						            		But in our case (noisy data) we don't what to fit the data exactly.
						            	</p>
						            </div>
						            <div class="col-md-8">
						            	<div class="thumbnail">
											<ul class="row">
												<li class="thumb">
								         			<img src="images/graphs/1DNoiseLS.png" height="384" width="512" class="img-responsive" alt="Responsive image">
								         		</li>
							      			</ul>
							      		</div>
						            </div>
						            <div class="col-md-4">
						            	<p class="text">
						            		The LSL1 algorithm visibly performs better. Instead of fitting the data exactly the algorithm is forced
						            		by the parameter \(\lambda = 0.8 \) to compute a sparse solution, which reduces the noise level. The error
						            		of the reconstruction is 2.3469 which is nearly the half of the error of the least squares reconstruction (4.5274)
						            	</p>
						            </div>
						            <div class="col-md-8">
						            	<div class="thumbnail">
											<ul class="row">
												<li class="thumb">
								         			<img src="images/graphs/1DNoiseLSL1.png" height="384" width="512" class="img-responsive" alt="Responsive image">
								         		</li>
							      			</ul>
							      		</div>
						            </div>
						            <h3>Playing with \(\lambda\)</h3>
						            <p class="text">
						            	In this section I tweak the parameter \(\lambda\) and computed the sparsity (\(L_1\)) of the coefficient vector 
						            	and the resulting error. The parameter is uniformly sampled from 0.0001 to 0.01. 
						            </p>
						            <div class="col-md-12">
						            	<div class="thumbnail">
											<ul class="row">
												<li class="thumb">
								         			<img src="images/graphs/1DLambda.png" height="464" width="827" class="img-responsive" alt="Responsive image">
								         		</li>
							      			</ul>
							      		</div>
						            </div>
						            <h3>2D function reconstruction with Gaussian RBF</h3>
						            <p class="text">
						            	Since the LSL1 algorithm achieved so promising result in the 1D case, we wanted to test the 
						            	robustness and the resulting reconstructions in 2D using the same function as in the previous section. 
						            	The following animation shows the results with increasing \(\lambda\) values.
						            </p>
						            
					            	<div class="fadein">
									    <img id="f4" src="images/graphs/2DLSL1DifferentLambda/LSL1_5.png">
									    <img id="f3" src="images/graphs/2DLSL1DifferentLambda/LSL1_2.5.png">
									    <img id="f2" src="images/graphs/2DLSL1DifferentLambda/LSL1_1.png">
									    <img id="f1" src="images/graphs/2DLSL1DifferentLambda/LSL1_0.1.png">
									</div>
									
						            <div class="col-md-12">
						            	<h3>\(\lambda = 0.1\) err = 0.2772</h3>
						            	<div id="2DLSL101"></div>
						            </div>
						            <div class="col-md-12">
						            	<h3>\(\lambda = 1.0\) err = 1.5574</h3>
						            	<div id="2DLSL110"></div>
						            </div>
						            <div class="col-md-12">
						            	<h3>\(\lambda = 2.5\) err = 2.8596</h3>
						            	<div id="2DLSL125"></div>
						            </div>
						            <div class="col-md-12">
						            	<h3>\(\lambda = 5.0\) err = 3.5322</h3>
						            	<div id="2DLSL15"></div>
						            </div>
						            <p class="text">
						            	The charts show us that the LSL1 algorithm is a promising candidate for the 3D case. 
						            </p>						           
				                </div>
				            </div>
				        </div>
                        <div class="panel panel-default">
                            <div class="panel-heading">
                                <h3 class="panel-title">
                                    <a data-toggle="collapse" data-parent="#accordion" href="#Conclusion">Conclusion and Outlook</a>
                                </h3>
                            </div>
                            <div id="Conclusion" class="panel-collapse collapse">
                                <div class="panel-body">
                                    <div class="col-md-12">
                                        <p class="text">
                                            We introduced a new surface reconstruction technique which combines classical dictionary
                                            learning and compressive sensing. First, the method computes a grid containing the signed
                                            distance to the scanned data and extracts the zero level surface. Our approach makes recon-
                                            struction with undersampled or noisy point clouds possible.
                                            In chapter 1 we started with a quick introduction and explained the importance of surface re-
                                            construction generally. In chapter 2 we gave a brief overview of the related work in the field of
                                            surface reconstruction, compressive sensing and dictionary learning. We discussed the advan-
                                            tages and disadvantages of the state-of-the-art methods and classified them into categories. In
                                            the next chapter we explained the theoretical background of our new reconstruction technique
                                            and step by step built the final minimization formulation. In our experiments we compared
                                            different sparse solvers in terms of run time, reconstruction error and sparsity. We found out
                                            that the L 1 regularized least squares minimization mostly fitted our needs. We started to test the
                                            reconstruction from the one dimensional to the three dimensional case and documented prob-
                                            lems we encountered during this process and how we solved them. Furthermore, we did a lot
                                            of testing to find the best relationship between the available parameters and found those few
                                            intuitive ones for the user. Finally, we can say that our method can keep up with the state-
                                            of-the-art methods with some requirements, like remembering and using the local coordinate
                                            frames. We contribute a modular reconstruction framework implemented in C++, which allows
                                            easily to change or extent different parts, like the weighting function or the solver, from the
                                            reconstruction process.
                                            Future work tackles the problem of the local frame approximation. To get more stable recon-
                                            structions, the local frames should not vary much if the number of points is decreased or noise
                                            is added to the model. A possible way would be to take the normals into account and use them
                                            to compute the local coordinate system. Furthermore, the neighbourhood querying needs to be
                                            improved, since in thin areas we get very spiky height fields resulting from the fact, that the 
                                            local surface is not topologically equal to a disc. This problem could be solved by taking the
                                            nearest point as a start point and using very small sphere radii for the queries to incrementally
                                            add points to the local points, if they are still in the demanded search radius. This approach
                                            would solve the problem, but also drastically increase the run time. Another suggestion to this
                                            problem would be to classify this situation as a problematic scenario and only use the samples
                                            of the closer side. However, also in sharp regions like in corners or edges the method fails to
                                            represent those and causes oversmoothing. To tackle this problem a different set of basis could
                                            be used, which can represent sharp features better than Gaussians. Finally, we want to create a
                                            better dictionary, which is constructed with training data from a big amount of different point
                                            clouds with different features and find out if we could get rid of some problems by optimizing
                                            the used dictionary.
                                        </p>
                                    </div>                                    
                                </div>
                            </div>
                        </div>
					</div>
				</div>
			</div>
		</div>
	</div>
	<div class="modal fade" id="myModal" tabindex="-1" role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
	  <div class="modal-dialog">
	    <div class="modal-content">         
	      <div class="modal-body">                
	      </div>
	    </div><!-- /.modal-content -->
	  </div><!-- /.modal-dialog -->
	</div><!-- /.modal -->
	<script>
    $(document).ready(function(){
    	$(function () {
		    $('#accordion').on('shown.bs.collapse', function (e) {
		        var offset = $('.panel.panel-default > .panel-collapse.in').offset();
		        if(offset) {
		            $('html,body').animate({
		                scrollTop: $('.panel-title a').offset().top -20
		            }, 500); 
		        }
		    }); 
		});
      $('#results1').diffWidget(
      {
        'width': '600px',
        'height': '550px',
        'top': 'images/results/bunnyDeg0.png',
        'bottom': 'images/results/bunnyDeg1.png',
        'position': '75%'
      })
      $('#results2').diffWidget(
      {
        'width': '600px',
        'height': '550px',
        'top': 'images/results/bunnyDeg1.png',
        'bottom': 'images/results/bunnyDeg2.png',
        'position': '75%'
      })
      $('#BasisPursuit').diffWidget(
      {
        'width': '500px',
        'height': '375px',
        'top': 'images/graphs/BasisPursuit-Signal.png',
        'bottom': 'images/graphs/OriginalSignal.png',
        'position': '75%'
      })
      $('#OMP').diffWidget(
      {
        'width': '500px',
        'height': '375px',
        'top': 'images/graphs/OMP-Signal.png',
        'bottom': 'images/graphs/OriginalSignal.png',
        'position': '75%'
      })
      $('#ROMP').diffWidget(
      {
        'width': '500px',
        'height': '375px',
        'top': 'images/graphs/ROMP-Signal.png',
        'bottom': 'images/graphs/OriginalSignal.png',
        'position': '75%'
      })
      $('#CoSaMP').diffWidget(
      {
        'width': '500px',
        'height': '375px',
        'top': 'images/graphs/CoSaMP-Signal.png',
        'bottom': 'images/graphs/OriginalSignal.png',
        'position': '75%'
      })
      $('#SubspacePursuit').diffWidget(
      {
        'width': '500px',
        'height': '375px',
        'top': 'images/graphs/SubspacePursuit-Signal.png',
        'bottom': 'images/graphs/OriginalSignal.png',
        'position': '75%'
      })
      $('#SL0').diffWidget(
      {
        'width': '500px',
        'height': '375px',
        'top': 'images/graphs/SL0-Signal.png',
        'bottom': 'images/graphs/OriginalSignal.png',
        'position': '75%'
      })
      $('#AMP').diffWidget(
      {
        'width': '500px',
        'height': '375px',
        'top': 'images/graphs/AMP-Signal.png',
        'bottom': 'images/graphs/OriginalSignal.png',
        'position': '75%'
      })
      $('#EMBP').diffWidget(
      {
        'width': '500px',
        'height': '375px',
        'top': 'images/graphs/EMBP-Signal.png',
        'bottom': 'images/graphs/OriginalSignal.png',
        'position': '75%'
      })
      $('#APSS1').diffWidget(
      {
        'width': '425px',
        'height': '425px',
        'top': 'images/APSS/bunnyMLS.png',
        'bottom': 'images/APSS/bunnyAPSS.png',
        'position': '75%'
      })
      $('#APSS2').diffWidget(
      {
        'width': '425px',
        'height': '425px',
        'top': 'images/APSS/houndAPSS.png',
        'bottom': 'images/APSS/houndMLS.png',
        'position': '75%'
      })
      $('#1DSignal').diffWidget(
      {
        'width': '512px',
        'height': '384px',
        'top': 'images/graphs/1DSignalRec.png',
        'bottom': 'images/graphs/1DSignal.png',
        'position': '75%'
      })
      $('#1DSignalCoeff').diffWidget(
      {
        'width': '512px',
        'height': '384px',
        'top': 'images/graphs/1DSignalSparseCoeff.png',
        'bottom': 'images/graphs/1DSignalLeastSquaresCoeff.png',
        'position': '75%'
      })
      $('#2DSignal').diffWidget(
      {
        'width': '683px',
        'height': '345px',
        'top': 'images/graphs/2DSignalRec.png',
        'bottom': 'images/graphs/2DSignal.png',
        'position': '75%'
      })
      $('#2DSignalCoeff').diffWidget(
      {
        'width': '683px',
        'height': '345px',
        'top': 'images/graphs/2DSignalSparseCoeff.png',
        'bottom': 'images/graphs/2DSignalLeastSquaresCoeff.png',
        'position': '75%'
      })
      $('#1DSignalLSL1').diffWidget(
      {
        'width': '512px',
        'height': '384px',
        'top': 'images/graphs/1DSignalLSL1Rec.png',
        'bottom': 'images/graphs/1DSignal.png',
        'position': '75%'
      })
      $('#1DSignalCoeffLSL1').diffWidget(
      {
        'width': '512px',
        'height': '384px',
        'top': 'images/graphs/1DLSL1CoeffVSLSCoeff.png',
        'bottom': 'images/graphs/1DL1QCCoeffVSLSCoeff.png',
        'position': '75%'
      })
      $('#LSL1VSL1QC').diffWidget(
      {
        'width': '512px',
        'height': '384px',
        'top': 'images/graphs/1DLSL1Coeff.png',
        'bottom': 'images/graphs/1DL1QCCoeff.png',
        'position': '75%'
      })
      $('#2DLSL101').diffWidget(
      {
        'width': '512px',
        'height': '384px',
        'top': 'images/graphs/2DLSL1DifferentLambda/Coeff_0.1.png',
        'bottom': 'images/graphs/2DLSL1DifferentLambda/Coeff_LS.png',
        'position': '75%'
      })
      $('#2DLSL110').diffWidget(
      {
        'width': '512px',
        'height': '384px',
        'top': 'images/graphs/2DLSL1DifferentLambda/Coeff_1.png',
        'bottom': 'images/graphs/2DLSL1DifferentLambda/Coeff_LS.png',
        'position': '75%'
      })
      $('#2DLSL125').diffWidget(
      {
        'width': '512px',
        'height': '384px',
        'top': 'images/graphs/2DLSL1DifferentLambda/Coeff_2.5.png',
        'bottom': 'images/graphs/2DLSL1DifferentLambda/Coeff_LS.png',
        'position': '75%'
      })
      $('#2DLSL15').diffWidget(
      {
        'width': '512px',
        'height': '384px',
        'top': 'images/graphs/2DLSL1DifferentLambda/Coeff_5.png',
        'bottom': 'images/graphs/2DLSL1DifferentLambda/Coeff_LS.png',
        'position': '75%'
      })


    })    
  </script>
</body>
</html>